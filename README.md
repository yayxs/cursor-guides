## 上下文

LLM 人工智能模型

Token 之前的 token 来预测下一个 token

上下文指的的是：token的输入和token的输出

![alt text](image.png)

意图上下文：

![](intent-context.png)

还有一种是状态上下文：

换句话说，是事实存在的。比如提供错误消息、控制台日志、图像和代码块

上下文信息通常是两部分得到的：1个是用户主动提供的，1个是Cursor 这个工具通过搜索代码等方式收集得到的。

模型会自动评估，结合对话，从当前文件或者其他文件中获取相关的信息

不过，必要的是用户手动明确已知的任务，而不是让模型自行猜测。

两个例子：

1、 修改一下按钮的圆角

2、把按钮的圆角修改为 10px

会出现问题的结合：上下文信息不足 + 模型的型号较为老旧比如 claude-3.5-sonnet

上文说到一部分是是通过用户提供的，可以使用 `@-symbol`

| @      |                      |
| ------ | -------------------- |
| @code  | 某一段代码           |
| @file  | 可能包含无关的上下文 |
| @foler | 大量无关的上下文     |

### 总结

提高编码的效果，需要确定你想要什么 & 已经存在的什么（比如控制台的网络请求结果）
过少的上下文（模型能力不足的情况）容易幻觉；或者过多的上下文又会稀释
